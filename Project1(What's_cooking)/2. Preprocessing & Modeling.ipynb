{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# -*-coding: utf-8-*-\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re, nltk\n",
    "from nltk.stem import WordNetLemmatizer     \n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import VotingClassifier, RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.cross_validation import train_test_split, StratifiedKFold\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from scipy.stats import sem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* train 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cuisine</th>\n",
       "      <th>id</th>\n",
       "      <th>ingredients</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>greek</td>\n",
       "      <td>10259</td>\n",
       "      <td>[romaine lettuce, black olives, grape tomatoes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>southern_us</td>\n",
       "      <td>25693</td>\n",
       "      <td>[plain flour, ground pepper, salt, tomatoes, g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>filipino</td>\n",
       "      <td>20130</td>\n",
       "      <td>[eggs, pepper, salt, mayonaise, cooking oil, g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>indian</td>\n",
       "      <td>22213</td>\n",
       "      <td>[water, vegetable oil, wheat, salt]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>indian</td>\n",
       "      <td>13162</td>\n",
       "      <td>[black pepper, shallots, cornflour, cayenne pe...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       cuisine     id                                        ingredients\n",
       "0        greek  10259  [romaine lettuce, black olives, grape tomatoes...\n",
       "1  southern_us  25693  [plain flour, ground pepper, salt, tomatoes, g...\n",
       "2     filipino  20130  [eggs, pepper, salt, mayonaise, cooking oil, g...\n",
       "3       indian  22213                [water, vegetable oil, wheat, salt]\n",
       "4       indian  13162  [black pepper, shallots, cornflour, cayenne pe..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_json('train.json')\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Ingredients ':' 단위로 나눠 주기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cuisine</th>\n",
       "      <th>id</th>\n",
       "      <th>ingredients</th>\n",
       "      <th>All_of_ingredients</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>greek</td>\n",
       "      <td>10259</td>\n",
       "      <td>[romaine lettuce, black olives, grape tomatoes...</td>\n",
       "      <td>romaine lettuce:black olives:grape tomatoes:ga...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>southern_us</td>\n",
       "      <td>25693</td>\n",
       "      <td>[plain flour, ground pepper, salt, tomatoes, g...</td>\n",
       "      <td>plain flour:ground pepper:salt:tomatoes:ground...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>filipino</td>\n",
       "      <td>20130</td>\n",
       "      <td>[eggs, pepper, salt, mayonaise, cooking oil, g...</td>\n",
       "      <td>eggs:pepper:salt:mayonaise:cooking oil:green c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>indian</td>\n",
       "      <td>22213</td>\n",
       "      <td>[water, vegetable oil, wheat, salt]</td>\n",
       "      <td>water:vegetable oil:wheat:salt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>indian</td>\n",
       "      <td>13162</td>\n",
       "      <td>[black pepper, shallots, cornflour, cayenne pe...</td>\n",
       "      <td>black pepper:shallots:cornflour:cayenne pepper...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       cuisine     id                                        ingredients  \\\n",
       "0        greek  10259  [romaine lettuce, black olives, grape tomatoes...   \n",
       "1  southern_us  25693  [plain flour, ground pepper, salt, tomatoes, g...   \n",
       "2     filipino  20130  [eggs, pepper, salt, mayonaise, cooking oil, g...   \n",
       "3       indian  22213                [water, vegetable oil, wheat, salt]   \n",
       "4       indian  13162  [black pepper, shallots, cornflour, cayenne pe...   \n",
       "\n",
       "                                  All_of_ingredients  \n",
       "0  romaine lettuce:black olives:grape tomatoes:ga...  \n",
       "1  plain flour:ground pepper:salt:tomatoes:ground...  \n",
       "2  eggs:pepper:salt:mayonaise:cooking oil:green c...  \n",
       "3                     water:vegetable oil:wheat:salt  \n",
       "4  black pepper:shallots:cornflour:cayenne pepper...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['All_of_ingredients'] = train['ingredients'].map(':'.join)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ingredients 전처리\n",
    "\n",
    "* target : cuisine => LabelEncoder로 전처리\n",
    "* Ingredients 중에 상표가 포함되어 있고 글자가 깨져 있는 것 => Tokenizer로 걸러주기\n",
    "* Ingredients 중에 tomatoes, tomato 처럼 같은 재료이지만 단수 복수 형태로 나뉘어 있음\n",
    "    => WordNetLemmatizer로 구분 짓게 안되게 만들기\n",
    "    \n",
    "## CountVectorizer, TfidfVectorizer로 전처리 성능 비교    \n",
    "1. CountVectorizer로 Preprocessing  \n",
    "\n",
    "2. TfidfVectorizer로 Preprocessing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stemmer = WordNetLemmatizer()\n",
    "\n",
    "def stem_tokens(tokens, stemmer):\n",
    "    stemming = [stemmer.lemmatize(ingredients) for ingredients in tokens]\n",
    "    return stemming\n",
    "\n",
    "def tokenizer(words):\n",
    "    filter_words = re.sub(r'[^a-zA-Z]', \" \", words)\n",
    "    tokens = nltk.word_tokenize(filter_words)\n",
    "    stems = stem_tokens(tokens, stemmer)\n",
    "    return stems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Ingredients 코퍼스를 만들고 탐색 후 의미 없다고 생각한 단어들 stop_words로 생성\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stop_words= ['brazil', 'best', 'ic', 'kim', 'alum', 'bushi','old', 'pla', 'wax', 'truv', 'tip', 'kha',\n",
    "           'petit','classic', 'tie', 'mole', 'diet', 'navel','preserv', 'ume', 'soi', 'uncle','bengali','shin',\n",
    "           'rom', 'southwest', 'jerk','cool','p','minute','added', 'port','rome','french','extra','bloody','black',\n",
    "           'tokyo','cap','edible', 'winter','kong', 'noir','hoi','texas','wagon','frank','non',\n",
    "           'farmer','artisan','rock','peasant','el','dutch', 'bragg','romano','cara','blood','rins',\n",
    "           'nutritional','fast','spanish','ring','sheet','white','season','thai','prime','enriched','helix',\n",
    "           'activ','wood','lotus','america','pain','concentrate','spare','vre','color','mark','single',\n",
    "           'hot', 'machine','greek','london','hidden','silver','ra','tot','moisture','tree','snow','m','di','dr','mex',\n",
    "           'n','seven','balance','cracked','split','hand','yellow','unflavored','asian','shaving','deli','rise','jack',\n",
    "           'softened','hero','cooking','dri','aka','golden', 'cane','elbow','mo','mi','mr','india','lan','green',\n",
    "           'imo','runny','navy','orang','ha','leav','eau','smart','well','plain','angled','korean','steel','farm','stock',\n",
    "           'challenge','baby','lea','long','trumpet','chunk','siu','tap','island','bird','curl','young','shape','pace','napa',\n",
    "           'believ','fiber','food','vie','haas','eye','te','oz','on','world','dark','summer','or','ruby','a','quick',\n",
    "           'head','o','jose','cortland','clarified','full','himalayan', 'free','japanese','holy','blue','new','straw','olek','shoot',\n",
    "           'earth', 'fire','acting','fri','lean','i','celtic','multi','ch','layer','well','gray','won','ngo','le','lb','la','lo',\n",
    "           'bing','imitation', 'pam', 'one','bibb','rich','cloud','cho','age','cup','kasu','swiss','ready','straight','yu','fu','jimmy'\n",
    "           'clear','self','aged','mountain','everglades','part','mission','rocket', 'cross','game','lower','devil','b',\n",
    "           'energy','style','good','hard','paper','brown','diamond','bag', 'bai', 'bar','bob', 'torn','not','mae', \n",
    "           'finger', 'submarine', 'chua', 'it', 'in', 'hong', 'hanh','tyson','with','pod', 'pop','angel','four']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CountVectorizer 사용\n",
    "\n",
    "* tokenizer 함수 적용\n",
    "* target = LabelEncoder 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(39774, 2783)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv = CountVectorizer(analyzer= 'word', tokenizer = tokenizer)\n",
    "ingredients_corpus = cv.fit_transform(train.All_of_ingredients.tolist())\n",
    "ingredients_corpus.shape \n",
    "# CountVectorizer로 진행하고 tokenizer 함수를 적용 시켰을 때 재료 수 : 2783개"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'unflavored', u'mackerel', u'yellow', u'sichuan', u'negi', u'clotted', u'asian', u'bucatini', u'hyssop', u'pancetta', u'shaving', u'manis', u'deli', u'rise', u'gremolata', u'jack', u'softened', u'icing', u'four', u'baking', u'broiler', u'wholemeal', u'acinus', u'chambord', u'frozen', u'moulard', u'cholesterol', u'poppy', u'uncooked', u'muscovy', u'orzo', u'jamaica', u'speck', u'clover', u'ravva', u'almond', u'bacon', u'japanese', u'millet', u'brill', u'soppressata', u'chee', u'cactus', u'blue', u'fontina', u'mascarpone', u'tipo', u'cooking', u'togarashi', u'salsify', u'galangal', u'new', u'zesty', u'crunch', u'hero', u'devein', u'kefalotyri', u'herb', u'jasmine', u'broccolini', u'marnier', u'floret', u'jamon', u'textured', u'active', u'johnsonville', u'mozzarella', u'dry', u'tumeric', u'lychee', u'stevia', u'dri', u'mezzetta', u'chopmeat', u'smoke', u'bertolli', u'aka', u'pack', u'golden', u'canola', u'straw', u'sliced', u'dungeness', u'cane', u'dumpling', u'soybean', u'shoyu', u'estancia', u'ploy', u'blueberri', u'spike', u'sticker', u'calf', u'grassfed', u'holy', u'sauvignon', u'oscar', u'dressing', u'padano', u'melba']\n"
     ]
    }
   ],
   "source": [
    "# Ingredients Corpus Shape\n",
    "\n",
    "print cv.vocabulary_.keys()[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "y = le.fit_transform(train['cuisine'])\n",
    "X_1 = ingredients_corpus \n",
    "#CountVectorizer, TfidfVectorizer 구분 지어주기 위해 X_1, X_2로 지정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Cross Validation 하기 위해 train, test로 Spliting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_1, y, train_size = 0.75, \n",
    "                                                    random_state = 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "logistic = LogisticRegression()\n",
    "mnb = MultinomialNB()\n",
    "svm = SVC(probability=True, kernel = 'linear')\n",
    "ens = VotingClassifier(estimators=[('lr', logistic), ('mnb', mnb), ('svm', svm)], voting='soft', weights = [3,1,1])\n",
    "\n",
    "model1 = logistic.fit(X_train, y_train)\n",
    "model2 = mnb.fit(X_train, y_train)\n",
    "model3 = svm.fit(X_train, y_train)\n",
    "model4 = ens.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python27\\lib\\site-packages\\ipykernel\\__main__.py:2: FutureWarning: order is deprecated, use sort_values(...)\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "# 알파벳 순으로 cuisine Sort, target_names 설정, LabelEncoder 상에는 0~19까지 알파벳 순으로 되어 있으므로\n",
    "cuisines = train.loc[train['cuisine'].str.lower().order().index]\n",
    "target_names = cuisines['cuisine'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "   brazilian       0.67      0.46      0.55       106\n",
      "     british       0.55      0.44      0.49       207\n",
      "cajun_creole       0.77      0.68      0.72       386\n",
      "     chinese       0.78      0.88      0.83       613\n",
      "    filipino       0.78      0.64      0.70       198\n",
      "      french       0.64      0.61      0.62       666\n",
      "       greek       0.80      0.72      0.76       290\n",
      "      indian       0.87      0.89      0.88       765\n",
      "       irish       0.65      0.46      0.54       189\n",
      "     italian       0.80      0.90      0.85      1914\n",
      "    jamaican       0.86      0.72      0.78       134\n",
      "    japanese       0.87      0.66      0.75       362\n",
      "      korean       0.82      0.77      0.79       213\n",
      "     mexican       0.89      0.93      0.91      1625\n",
      "    moroccan       0.83      0.76      0.79       212\n",
      "     russian       0.60      0.39      0.48       132\n",
      " southern_us       0.68      0.81      0.74      1084\n",
      "     spanish       0.70      0.50      0.59       244\n",
      "        thai       0.80      0.75      0.78       399\n",
      "  vietnamese       0.67      0.48      0.56       205\n",
      "\n",
      " avg / total       0.78      0.78      0.78      9944\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# logistic Report\n",
    "y_pred = model1.predict(X_test)\n",
    "print classification_report(y_test, y_pred, target_names = target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "   brazilian       0.50      0.27      0.35       106\n",
      "     british       0.30      0.42      0.35       207\n",
      "cajun_creole       0.54      0.74      0.62       386\n",
      "     chinese       0.70      0.89      0.78       613\n",
      "    filipino       0.71      0.46      0.56       198\n",
      "      french       0.56      0.53      0.55       666\n",
      "       greek       0.67      0.60      0.63       290\n",
      "      indian       0.85      0.87      0.86       765\n",
      "       irish       0.63      0.35      0.45       189\n",
      "     italian       0.84      0.80      0.82      1914\n",
      "    jamaican       0.84      0.57      0.68       134\n",
      "    japanese       0.85      0.56      0.68       362\n",
      "      korean       0.81      0.70      0.75       213\n",
      "     mexican       0.90      0.88      0.89      1625\n",
      "    moroccan       0.74      0.75      0.74       212\n",
      "     russian       0.54      0.23      0.33       132\n",
      " southern_us       0.56      0.69      0.62      1084\n",
      "     spanish       0.53      0.55      0.54       244\n",
      "        thai       0.70      0.77      0.73       399\n",
      "  vietnamese       0.63      0.46      0.54       205\n",
      "\n",
      " avg / total       0.73      0.72      0.72      9944\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# MultinomialNB Report\n",
    "y_pred = model2.predict(X_test)\n",
    "print classification_report(y_test, y_pred, target_names = target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "   brazilian       0.54      0.52      0.53       106\n",
      "     british       0.48      0.52      0.50       207\n",
      "cajun_creole       0.74      0.71      0.72       386\n",
      "     chinese       0.74      0.85      0.79       613\n",
      "    filipino       0.67      0.63      0.65       198\n",
      "      french       0.59      0.62      0.61       666\n",
      "       greek       0.74      0.70      0.72       290\n",
      "      indian       0.87      0.87      0.87       765\n",
      "       irish       0.58      0.49      0.53       189\n",
      "     italian       0.81      0.87      0.84      1914\n",
      "    jamaican       0.80      0.67      0.73       134\n",
      "    japanese       0.80      0.68      0.73       362\n",
      "      korean       0.83      0.75      0.79       213\n",
      "     mexican       0.90      0.89      0.90      1625\n",
      "    moroccan       0.81      0.78      0.80       212\n",
      "     russian       0.54      0.37      0.44       132\n",
      " southern_us       0.72      0.77      0.75      1084\n",
      "     spanish       0.66      0.50      0.57       244\n",
      "        thai       0.78      0.73      0.75       399\n",
      "  vietnamese       0.61      0.48      0.54       205\n",
      "\n",
      " avg / total       0.77      0.77      0.77      9944\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# SVM Report\n",
    "y_pred = model3.predict(X_test)\n",
    "print classification_report(y_test, y_pred, target_names = target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "   brazilian       0.65      0.43      0.52       106\n",
      "     british       0.49      0.45      0.47       207\n",
      "cajun_creole       0.75      0.70      0.73       386\n",
      "     chinese       0.76      0.89      0.82       613\n",
      "    filipino       0.81      0.62      0.70       198\n",
      "      french       0.63      0.60      0.62       666\n",
      "       greek       0.80      0.69      0.74       290\n",
      "      indian       0.88      0.90      0.89       765\n",
      "       irish       0.71      0.45      0.55       189\n",
      "     italian       0.82      0.90      0.86      1914\n",
      "    jamaican       0.90      0.69      0.78       134\n",
      "    japanese       0.87      0.66      0.75       362\n",
      "      korean       0.84      0.76      0.80       213\n",
      "     mexican       0.90      0.92      0.91      1625\n",
      "    moroccan       0.80      0.75      0.77       212\n",
      "     russian       0.65      0.34      0.45       132\n",
      " southern_us       0.66      0.81      0.73      1084\n",
      "     spanish       0.70      0.54      0.61       244\n",
      "        thai       0.79      0.78      0.79       399\n",
      "  vietnamese       0.70      0.50      0.58       205\n",
      "\n",
      " avg / total       0.78      0.78      0.78      9944\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# VotingClassifier Report\n",
    "y_pred = model4.predict(X_test)\n",
    "print classification_report(y_test, y_pred, target_names = target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CountVectorizer로 전처리 한 후 Cross Validation(StratifiedKFold 방법으로 했을 때 각 모델별 Accuarcy Score)\n",
    "\n",
    "* 여기서 score는 Accuarcy를 의미"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0: Mean score: 0.785 (+/-0.002)\n",
      "Model 1: Mean score: 0.722 (+/-0.002)\n",
      "Model 2: Mean score: 0.771 (+/-0.001)\n",
      "Model 3: Mean score: 0.786 (+/-0.002)\n"
     ]
    }
   ],
   "source": [
    "# stratified K-Fold 5번\n",
    "for i, clf in enumerate([logistic, mnb, svm, ens]):\n",
    "    scores = cross_val_score(clf, X_1, y, cv=5)\n",
    "    print((\"Model {0:d}: Mean score: {1:.3f} (+/-{2:.3f})\").format(i, np.mean(scores), sem(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TfidfVectorizer 사용\n",
    "\n",
    "* tokenizer 함수 적용\n",
    "* target = LabelEncoder 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tv = TfidfVectorizer(analyzer= 'word', tokenizer = tokenizer)\n",
    "ingredients_corpus = tv.fit_transform(train.All_of_ingredients.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "le = LabelEncoder() \n",
    "y = le.fit_transform(train['cuisine'])\n",
    "X_2 = ingredients_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_2, y, train_size = 0.75, \n",
    "                                                   random_state = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logistic = LogisticRegression()\n",
    "mnb = MultinomialNB()\n",
    "svm = SVC(probability=True, kernel = 'linear')\n",
    "ens = VotingClassifier(estimators=[('lr', logistic), ('mnb', mnb), ('svm', svm)], voting='soft', weights = [3,1,1])\n",
    "\n",
    "model5 = logistic.fit(X_train, y_train)\n",
    "model6 = mnb.fit(X_train, y_train)\n",
    "model7 = svm.fit(X_train, y_train)\n",
    "model8 = ens.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "   brazilian       0.61      0.35      0.44       106\n",
      "     british       0.58      0.35      0.44       207\n",
      "cajun_creole       0.78      0.67      0.72       386\n",
      "     chinese       0.74      0.89      0.81       613\n",
      "    filipino       0.78      0.56      0.65       198\n",
      "      french       0.60      0.59      0.59       666\n",
      "       greek       0.85      0.64      0.73       290\n",
      "      indian       0.86      0.90      0.88       765\n",
      "       irish       0.73      0.40      0.52       189\n",
      "     italian       0.76      0.91      0.83      1914\n",
      "    jamaican       0.91      0.64      0.75       134\n",
      "    japanese       0.89      0.62      0.73       362\n",
      "      korean       0.83      0.73      0.78       213\n",
      "     mexican       0.88      0.93      0.91      1625\n",
      "    moroccan       0.86      0.72      0.78       212\n",
      "     russian       0.88      0.28      0.43       132\n",
      " southern_us       0.66      0.82      0.73      1084\n",
      "     spanish       0.70      0.46      0.55       244\n",
      "        thai       0.80      0.77      0.79       399\n",
      "  vietnamese       0.72      0.45      0.55       205\n",
      "\n",
      " avg / total       0.78      0.77      0.76      9944\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# logistic Report\n",
    "y_pred = model5.predict(X_test)\n",
    "print classification_report(y_test, y_pred, target_names = target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "   brazilian       0.83      0.14      0.24       106\n",
      "     british       0.75      0.06      0.11       207\n",
      "cajun_creole       0.78      0.51      0.62       386\n",
      "     chinese       0.55      0.93      0.70       613\n",
      "    filipino       0.95      0.10      0.17       198\n",
      "      french       0.56      0.40      0.47       666\n",
      "       greek       0.89      0.30      0.45       290\n",
      "      indian       0.79      0.91      0.84       765\n",
      "       irish       1.00      0.05      0.10       189\n",
      "     italian       0.64      0.91      0.75      1914\n",
      "    jamaican       1.00      0.16      0.27       134\n",
      "    japanese       0.94      0.50      0.65       362\n",
      "      korean       0.99      0.32      0.49       213\n",
      "     mexican       0.80      0.92      0.86      1625\n",
      "    moroccan       0.96      0.34      0.50       212\n",
      "     russian       1.00      0.02      0.03       132\n",
      " southern_us       0.49      0.77      0.60      1084\n",
      "     spanish       0.96      0.11      0.19       244\n",
      "        thai       0.70      0.72      0.71       399\n",
      "  vietnamese       0.91      0.16      0.27       205\n",
      "\n",
      " avg / total       0.73      0.67      0.62      9944\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# MultinomialNB Report\n",
    "y_pred = model6.predict(X_test)\n",
    "print classification_report(y_test, y_pred, target_names = target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "   brazilian       0.63      0.48      0.55       106\n",
      "     british       0.54      0.47      0.51       207\n",
      "cajun_creole       0.77      0.70      0.73       386\n",
      "     chinese       0.76      0.88      0.82       613\n",
      "    filipino       0.75      0.64      0.69       198\n",
      "      french       0.61      0.63      0.62       666\n",
      "       greek       0.82      0.71      0.76       290\n",
      "      indian       0.87      0.89      0.88       765\n",
      "       irish       0.68      0.45      0.54       189\n",
      "     italian       0.79      0.90      0.84      1914\n",
      "    jamaican       0.91      0.68      0.78       134\n",
      "    japanese       0.86      0.64      0.74       362\n",
      "      korean       0.85      0.77      0.81       213\n",
      "     mexican       0.90      0.91      0.91      1625\n",
      "    moroccan       0.85      0.75      0.80       212\n",
      "     russian       0.64      0.33      0.43       132\n",
      " southern_us       0.70      0.80      0.75      1084\n",
      "     spanish       0.73      0.50      0.59       244\n",
      "        thai       0.82      0.78      0.80       399\n",
      "  vietnamese       0.74      0.50      0.60       205\n",
      "\n",
      " avg / total       0.78      0.78      0.78      9944\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# SVM Report\n",
    "y_pred = model7.predict(X_test)\n",
    "print classification_report(y_test, y_pred, target_names = target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "   brazilian       0.73      0.41      0.52       106\n",
      "     british       0.63      0.34      0.44       207\n",
      "cajun_creole       0.79      0.65      0.72       386\n",
      "     chinese       0.74      0.91      0.81       613\n",
      "    filipino       0.82      0.54      0.65       198\n",
      "      french       0.63      0.60      0.62       666\n",
      "       greek       0.85      0.62      0.72       290\n",
      "      indian       0.86      0.90      0.88       765\n",
      "       irish       0.77      0.40      0.52       189\n",
      "     italian       0.76      0.92      0.83      1914\n",
      "    jamaican       0.92      0.65      0.76       134\n",
      "    japanese       0.89      0.64      0.74       362\n",
      "      korean       0.86      0.72      0.78       213\n",
      "     mexican       0.88      0.93      0.91      1625\n",
      "    moroccan       0.88      0.73      0.79       212\n",
      "     russian       0.84      0.28      0.42       132\n",
      " southern_us       0.64      0.82      0.72      1084\n",
      "     spanish       0.76      0.45      0.57       244\n",
      "        thai       0.80      0.78      0.79       399\n",
      "  vietnamese       0.74      0.45      0.56       205\n",
      "\n",
      " avg / total       0.78      0.78      0.77      9944\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# VotingClassifier Report\n",
    "y_pred = model8.predict(X_test)\n",
    "print classification_report(y_test, y_pred, target_names = target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TfidfVectorizer로 전처리 한 후 Cross Validation(StratifiedKFold 방법으로 했을 때 각 모델별 Accuarcy Score)\n",
    "\n",
    "* 여기서 Score는 Accuracy를 의미"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0: Mean score: 0.776 (+/-0.001)\n",
      "Model 1: Mean score: 0.672 (+/-0.003)\n",
      "Model 2: Mean score: 0.786 (+/-0.002)\n",
      "Model 3: Mean score: 0.779 (+/-0.002)\n"
     ]
    }
   ],
   "source": [
    "for i, clf in enumerate([logistic, mnb, svm, ens]):\n",
    "    scores = cross_val_score(clf, X_2, y, cv=5)\n",
    "    print((\"Model {0:d}: Mean score: {1:.3f} (+/-{2:.3f})\").format(i, np.mean(scores), sem(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
